[info] logging to /Users/draxlindgren/Downloads/my_local_ai/logs/session-20251210-061657.log
[info] session started 2025-12-10T06:16:57.993864 -> /Users/draxlindgren/Downloads/my_local_ai/logs/session-20251210-061657.log
You: You: yo
AI: AI: Thinkingâ€¦ 0.2s |AI: Thinkingâ€¦ 0.4s /AI: Thinkingâ€¦ 0.6s -AI: Thinkingâ€¦ 0.8s \AI: Thinkingâ€¦ 1.0s |AI: Thinkingâ€¦ 1.2s /AI: Thinkingâ€¦ 1.4s -AI: Thinkingâ€¦ 1.6s \AI: Thinkingâ€¦ 1.8s |AI: Thinkingâ€¦ 2.0s /AI: Thinkingâ€¦ 2.2s -AI: Thinkingâ€¦ 2.4s \AI: Thinkingâ€¦ 2.6s |AI: Thinkingâ€¦ 2.8s /AI: Thinkingâ€¦ 3.0s -AI: Thinkingâ€¦ 3.2s \AI: Thinkingâ€¦ 3.4s |AI: Thinkingâ€¦ 3.6s /AI: Thinkingâ€¦ 3.8s -AI: Thinkingâ€¦ 4.0s \AI: Thinkingâ€¦ 4.2s |AI: Thinkingâ€¦ 4.4s /AI: Thinkingâ€¦ 4.6s -AI: Thinkingâ€¦ 4.8s \AI: Thinkingâ€¦ 5.0s |AI: Thinkingâ€¦ 5.2s /AI: Thinkingâ€¦ 5.4s -AI: Thinkingâ€¦ 5.6s \AI: Thinkingâ€¦ 5.8s |AI: Thinkingâ€¦ 6.0s /AI: Thinkingâ€¦ 6.2s -AI: Thinkingâ€¦ 6.4s \AI: Thinkingâ€¦ 6.6s |AI: Thinkingâ€¦ 6.8s /AI: Thinkingâ€¦ 7.1s -AI: Thinkingâ€¦ 7.3s \AI: Thinkingâ€¦ 7.5s |AI: Thinkingâ€¦ 7.7s /AI: Thinkingâ€¦ 7.9s -AI: Thinkingâ€¦ 8.1s \AI: Thinkingâ€¦ 8.3s |AI: Thinkingâ€¦ 8.5s /AI: Thinkingâ€¦ 8.7s -AI: Thinkingâ€¦ 8.9s \AI: Thinkingâ€¦ 9.1s |AI: Thinkingâ€¦ 9.3s /AI: Thinkingâ€¦ 9.5s -AI: Thinkingâ€¦ 9.7s \AI: Thinkingâ€¦ 9.9s |AI: Thinkingâ€¦ 10.1s /AI: Thinkingâ€¦ 10.3s -AI: Thinkingâ€¦ 10.5s \                                                                                AI: ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.009 sec
ggml_metal_device_init: GPU name:   Apple M1 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 51539.61 MB
build: 7073 (1411d9275) with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin23.6.0
main: llama backend init
main: load the model and apply lora adapter, if any
llama_model_load_from_file_impl: using device Metal (Apple M1 Max) (unknown id) - 49151 MiB free
llama_model_loader: loaded meta data with 40 key-value pairs and 363 tensors from /Users/draxlindgren/Downloads/my_local_ai/model/dolphin-2.9.3-mistral-nemo-12b.Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Quantized with KoboldCpp
llama_model_loader: - kv   3:                            general.version str              = 2407
llama_model_loader: - kv   4:                       general.organization str              = Mistralai
llama_model_loader: - kv   5:                           general.basename str              = Mistral-Nemo-Base
llama_model_loader: - kv   6:                         general.size_label str              = 12B
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Mistral Nemo Base 2407
llama_model_loader: - kv  10:               general.base_model.0.version str              = 2407
llama_model_loader: - kv  11:          general.base_model.0.organization str              = Mistralai
llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/mistralai/Mist...
llama_model_loader: - kv  13:                               general.tags arr[str,2]       = ["generated_from_trainer", "axolotl"]
llama_model_loader: - kv  14:                           general.datasets arr[str,8]       = ["cognitivecomputations/Dolphin-2.9",...
llama_model_loader: - kv  15:                          llama.block_count u32              = 40
llama_model_loader: - kv  16:                       llama.context_length u32              = 1024000
llama_model_loader: - kv  17:                     llama.embedding_length u32              = 5120
llama_model_loader: - kv  18:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  19:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  20:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  21:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  22:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  23:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  24:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - kv  26:                           llama.vocab_size u32              = 131074
llama_model_loader: - kv  27:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  28:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  29:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  30:                         tokenizer.ggml.pre str              = tekken
llama_model_loader: - kv  31:                      tokenizer.ggml.tokens arr[str,131074]  = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  32:                  tokenizer.ggml.token_type arr[i32,131074]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  33:                      tokenizer.ggml.merges arr[str,269443]  = ["Ä  Ä ", "Ä  t", "e r", "i n", "Ä  ï¿½...
llama_model_loader: - kv  34:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  35:                tokenizer.ggml.eos_token_id u32              = 131072
llama_model_loader: - kv  36:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  37:            tokenizer.ggml.padding_token_id u32              = 10
llama_model_loader: - kv  38:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  39:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q4_K:  241 tensors
llama_model_loader: - type q6_K:   41 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 6.96 GiB (4.88 BPW)
load: printing all EOG tokens:
load:   - 131072 ('')
load: special tokens cache size = 1002
load: token to piece cache size = 0.8499 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 1024000
print_info: n_embd           = 5120
print_info: n_embd_inp       = 5120
print_info: n_layer          = 40
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 1024000
print_info: rope_finetuned   = unknown
print_info: model type       = 13B
print_info: model params     = 12.25 B
print_info: general.name     = Quantized with KoboldCpp
print_info: vocab type       = BPE
print_info: n_vocab          = 131074
print_info: n_merges         = 269443
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 131072 ''
print_info: EOT token        = 131072 ''
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 10 '<pad>'
print_info: LF token         = 1010 'ÄŠ'
print_info: EOG token        = 131072 ''
print_info: max token length = 150
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 40 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 41/41 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   360.01 MiB
load_tensors: Metal_Mapped model buffer size =  6763.31 MiB
..........................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (1024000) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M1 Max
ggml_metal_init: picking default device: Apple M1 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =   640.00 MiB
llama_kv_cache: size =  640.00 MiB (  4096 cells,  40 layers,  1/1 seqs), K (f16):  320.00 MiB, V (f16):  320.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   266.00 MiB
llama_context:        CPU compute buffer size =    18.01 MiB
llama_context: graph nodes  = 1247
llama_context: graph splits = 2
common_init_from_params: added  logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main: llama threadpool init, n_threads = 8

system_info: n_threads = 8 (n_threads_batch = 8) / 10 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 |

sampler seed: 3499721564
sampler params:
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
generate: n_ctx = 4096, n_batch = 2048, n_predict = 300, n_keep = 1

, itâ€™s been a while.

yup.

iâ€™ve got a bit of a backlog here.  iâ€™ve been making stuff â€“ lots and lots of stuff â€“ but havenâ€™t really put the effort into documenting it allâ€¦ so today is going to be a bit of a catch-up post.  letâ€™s do this.

first up, a little while ago i decided that i needed some new shirts for work.  nothing too fancy or flashy â€“ just something more fitted and a bit more exciting than your standard button-down shirt from the mall.  so i went hunting through my fabric stash (which is still entirely too small) and came across this beautiful, light-weight cotton with a subtle geometric pattern on it.

i decided to go for a simple design that could be easily dressed up or down depending on what pants/skirt i paired it with:  the â€˜evelynâ€™ blouse from sewsavvy patterns.  this is a shirt-dress style that hits at about hip-length and has three-quarter length sleeves, with a faux-wrap front, hidden button placket, and side seam pockets.  itâ€™s also got an elastic waistband in the back for a bit of extra comfort.

the pattern comes together really easily (i used it as part of my sewing challenge for the month of may â€“ if you want to know more about that, check out #sewingmaychallenge over on instagram), and i was able to finish it up in

llama_perf_sampler_print:    sampling time =     124.60 ms /   302 runs   (    0.41 ms per token,  2423.72 tokens per second)
llama_perf_context_print:        load time =     521.57 ms
llama_perf_context_print: prompt eval time =      59.13 ms /     2 tokens (   29.57 ms per token,    33.82 tokens per second)
llama_perf_context_print:        eval time =    9588.88 ms /   299 runs   (   32.07 ms per token,    31.18 tokens per second)
llama_perf_context_print:       total time =    9790.69 ms /   301 tokens
llama_perf_context_print:    graphs reused =        298
llama_memory_breakdown_print: | memory breakdown [MiB]   | total    free    self   model   context   compute    unaccounted |
llama_memory_breakdown_print: |   - Metal (Apple M1 Max) | 49152 = 41482 + (7669 =  6763 +     640 +     266) +           0 |
llama_memory_breakdown_print: |   - Host                 |                   378 =   360 +       0 +      18                |
ggml_metal_free: deallocatingAI: ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.009 sec
ggml_metal_device_init: GPU name:   Apple M1 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 51539.61 MB
build: 7073 (1411d9275) with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin23.6.0
main: llama backend init
main: load the model and apply lora adapter, if any
llama_model_load_from_file_impl: using device Metal (Apple M1 Max) (unknown id) - 49151 MiB free
llama_model_loader: loaded meta data with 40 key-value pairs and 363 tensors from /Users/draxlindgren/Downloads/my_local_ai/model/dolphin-2.9.3-mistral-nemo-12b.Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Quantized with KoboldCpp
llama_model_loader: - kv   3:                            general.version str              = 2407
llama_model_loader: - kv   4:                       general.organization str              = Mistralai
llama_model_loader: - kv   5:                           general.basename str              = Mistral-Nemo-Base
llama_model_loader: - kv   6:                         general.size_label str              = 12B
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Mistral Nemo Base 2407
llama_model_loader: - kv  10:               general.base_model.0.version str              = 2407
llama_model_loader: - kv  11:          general.base_model.0.organization str              = Mistralai
llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/mistralai/Mist...
llama_model_loader: - kv  13:                               general.tags arr[str,2]       = ["generated_from_trainer", "axolotl"]
llama_model_loader: - kv  14:                           general.datasets arr[str,8]       = ["cognitivecomputations/Dolphin-2.9",...
llama_model_loader: - kv  15:                          llama.block_count u32              = 40
llama_model_loader: - kv  16:                       llama.context_length u32              = 1024000
llama_model_loader: - kv  17:                     llama.embedding_length u32              = 5120
llama_model_loader: - kv  18:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  19:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  20:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  21:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  22:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  23:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  24:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - kv  26:                           llama.vocab_size u32              = 131074
llama_model_loader: - kv  27:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  28:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  29:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  30:                         tokenizer.ggml.pre str              = tekken
llama_model_loader: - kv  31:                      tokenizer.ggml.tokens arr[str,131074]  = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  32:                  tokenizer.ggml.token_type arr[i32,131074]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  33:                      tokenizer.ggml.merges arr[str,269443]  = ["Ä  Ä ", "Ä  t", "e r", "i n", "Ä  ï¿½...
llama_model_loader: - kv  34:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  35:                tokenizer.ggml.eos_token_id u32              = 131072
llama_model_loader: - kv  36:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  37:            tokenizer.ggml.padding_token_id u32              = 10
llama_model_loader: - kv  38:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  39:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q4_K:  241 tensors
llama_model_loader: - type q6_K:   41 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 6.96 GiB (4.88 BPW)
load: printing all EOG tokens:
load:   - 131072 ('')
load: special tokens cache size = 1002
load: token to piece cache size = 0.8499 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 1024000
print_info: n_embd           = 5120
print_info: n_embd_inp       = 5120
print_info: n_layer          = 40
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 1024000
print_info: rope_finetuned   = unknown
print_info: model type       = 13B
print_info: model params     = 12.25 B
print_info: general.name     = Quantized with KoboldCpp
print_info: vocab type       = BPE
print_info: n_vocab          = 131074
print_info: n_merges         = 269443
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 131072 ''
print_info: EOT token        = 131072 ''
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 10 '<pad>'
print_info: LF token         = 1010 'ÄŠ'
print_info: EOG token        = 131072 ''
print_info: max token length = 150
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 40 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 41/41 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   360.01 MiB
load_tensors: Metal_Mapped model buffer size =  6763.31 MiB
..........................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (1024000) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M1 Max
ggml_metal_init: picking default device: Apple M1 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =   640.00 MiB
llama_kv_cache: size =  640.00 MiB (  4096 cells,  40 layers,  1/1 seqs), K (f16):  320.00 MiB, V (f16):  320.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   266.00 MiB
llama_context:        CPU compute buffer size =    18.01 MiB
llama_context: graph nodes  = 1247
llama_context: graph splits = 2
common_init_from_params: added  logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main: llama threadpool init, n_threads = 8

system_info: n_threads = 8 (n_threads_batch = 8) / 10 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 |

sampler seed: 3499721564
sampler params:
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
generate: n_ctx = 4096, n_batch = 2048, n_predict = 300, n_keep = 1

, itâ€™s been a while.

yup.

iâ€™ve got a bit of a backlog here.  iâ€™ve been making stuff â€“ lots and lots of stuff â€“ but havenâ€™t really put the effort into documenting it allâ€¦ so today is going to be a bit of a catch-up post.  letâ€™s do this.

first up, a little while ago i decided that i needed some new shirts for work.  nothing too fancy or flashy â€“ just something more fitted and a bit more exciting than your standard button-down shirt from the mall.  so i went hunting through my fabric stash (which is still entirely too small) and came across this beautiful, light-weight cotton with a subtle geometric pattern on it.

i decided to go for a simple design that could be easily dressed up or down depending on what pants/skirt i paired it with:  the â€˜evelynâ€™ blouse from sewsavvy patterns.  this is a shirt-dress style that hits at about hip-length and has three-quarter length sleeves, with a faux-wrap front, hidden button placket, and side seam pockets.  itâ€™s also got an elastic waistband in the back for a bit of extra comfort.

the pattern comes together really easily (i used it as part of my sewing challenge for the month of may â€“ if you want to know more about that, check out #sewingmaychallenge over on instagram), and i was able to finish it up in

llama_perf_sampler_print:    sampling time =     124.60 ms /   302 runs   (    0.41 ms per token,  2423.72 tokens per second)
llama_perf_context_print:        load time =     521.57 ms
llama_perf_context_print: prompt eval time =      59.13 ms /     2 tokens (   29.57 ms per token,    33.82 tokens per second)
llama_perf_context_print:        eval time =    9588.88 ms /   299 runs   (   32.07 ms per token,    31.18 tokens per second)
llama_perf_context_print:       total time =    9790.69 ms /   301 tokens
llama_perf_context_print:    graphs reused =        298
llama_memory_breakdown_print: | memory breakdown [MiB]   | total    free    self   model   context   compute    unaccounted |
llama_memory_breakdown_print: |   - Metal (Apple M1 Max) | 49152 = 41482 + (7669 =  6763 +     640 +     266) +           0 |
llama_memory_breakdown_print: |   - Host                 |                   378 =   360 +       0 +      18                |
ggml_metal_free: deallocating


You: You: hey, youre doing great hang in there
AI: AI: Thinkingâ€¦ 0.2s |AI: Thinkingâ€¦ 0.4s /AI: Thinkingâ€¦ 0.6s -AI: Thinkingâ€¦ 0.8s \AI: Thinkingâ€¦ 1.0s |AI: Thinkingâ€¦ 1.2s /AI: Thinkingâ€¦ 1.4s -AI: Thinkingâ€¦ 1.6s \AI: Thinkingâ€¦ 1.8s |AI: Thinkingâ€¦ 2.0s /AI: Thinkingâ€¦ 2.2s -AI: Thinkingâ€¦ 2.4s \AI: Thinkingâ€¦ 2.6s |AI: Thinkingâ€¦ 2.8s /AI: Thinkingâ€¦ 3.0s -AI: Thinkingâ€¦ 3.2s \AI: Thinkingâ€¦ 3.4s |AI: Thinkingâ€¦ 3.6s /AI: Thinkingâ€¦ 3.8s -AI: Thinkingâ€¦ 4.0s \AI: Thinkingâ€¦ 4.2s |AI: Thinkingâ€¦ 4.4s /AI: Thinkingâ€¦ 4.6s -AI: Thinkingâ€¦ 4.8s \AI: Thinkingâ€¦ 5.0s |AI: Thinkingâ€¦ 5.2s /AI: Thinkingâ€¦ 5.4s -AI: Thinkingâ€¦ 5.6s \AI: Thinkingâ€¦ 5.8s |AI: Thinkingâ€¦ 6.0s /AI: Thinkingâ€¦ 6.2s -AI: Thinkingâ€¦ 6.4s \AI: Thinkingâ€¦ 6.6s |AI: Thinkingâ€¦ 6.8s /AI: Thinkingâ€¦ 7.1s -AI: Thinkingâ€¦ 7.3s \AI: Thinkingâ€¦ 7.5s |AI: Thinkingâ€¦ 7.7s /AI: Thinkingâ€¦ 7.9s -AI: Thinkingâ€¦ 8.1s \AI: Thinkingâ€¦ 8.3s |AI: Thinkingâ€¦ 8.5s /AI: Thinkingâ€¦ 8.7s -AI: Thinkingâ€¦ 8.9s \AI: Thinkingâ€¦ 9.1s |AI: Thinkingâ€¦ 9.3s /AI: Thinkingâ€¦ 9.5s -AI: Thinkingâ€¦ 9.7s \AI: Thinkingâ€¦ 9.9s |AI: Thinkingâ€¦ 10.1s /AI: Thinkingâ€¦ 10.3s -                                                                                AI: ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.008 sec
ggml_metal_device_init: GPU name:   Apple M1 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 51539.61 MB
build: 7073 (1411d9275) with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin23.6.0
main: llama backend init
main: load the model and apply lora adapter, if any
llama_model_load_from_file_impl: using device Metal (Apple M1 Max) (unknown id) - 49151 MiB free
llama_model_loader: loaded meta data with 40 key-value pairs and 363 tensors from /Users/draxlindgren/Downloads/my_local_ai/model/dolphin-2.9.3-mistral-nemo-12b.Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Quantized with KoboldCpp
llama_model_loader: - kv   3:                            general.version str              = 2407
llama_model_loader: - kv   4:                       general.organization str              = Mistralai
llama_model_loader: - kv   5:                           general.basename str              = Mistral-Nemo-Base
llama_model_loader: - kv   6:                         general.size_label str              = 12B
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Mistral Nemo Base 2407
llama_model_loader: - kv  10:               general.base_model.0.version str              = 2407
llama_model_loader: - kv  11:          general.base_model.0.organization str              = Mistralai
llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/mistralai/Mist...
llama_model_loader: - kv  13:                               general.tags arr[str,2]       = ["generated_from_trainer", "axolotl"]
llama_model_loader: - kv  14:                           general.datasets arr[str,8]       = ["cognitivecomputations/Dolphin-2.9",...
llama_model_loader: - kv  15:                          llama.block_count u32              = 40
llama_model_loader: - kv  16:                       llama.context_length u32              = 1024000
llama_model_loader: - kv  17:                     llama.embedding_length u32              = 5120
llama_model_loader: - kv  18:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  19:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  20:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  21:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  22:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  23:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  24:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - kv  26:                           llama.vocab_size u32              = 131074
llama_model_loader: - kv  27:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  28:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  29:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  30:                         tokenizer.ggml.pre str              = tekken
llama_model_loader: - kv  31:                      tokenizer.ggml.tokens arr[str,131074]  = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  32:                  tokenizer.ggml.token_type arr[i32,131074]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  33:                      tokenizer.ggml.merges arr[str,269443]  = ["Ä  Ä ", "Ä  t", "e r", "i n", "Ä  ï¿½...
llama_model_loader: - kv  34:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  35:                tokenizer.ggml.eos_token_id u32              = 131072
llama_model_loader: - kv  36:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  37:            tokenizer.ggml.padding_token_id u32              = 10
llama_model_loader: - kv  38:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  39:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q4_K:  241 tensors
llama_model_loader: - type q6_K:   41 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 6.96 GiB (4.88 BPW)
load: printing all EOG tokens:
load:   - 131072 ('')
load: special tokens cache size = 1002
load: token to piece cache size = 0.8499 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 1024000
print_info: n_embd           = 5120
print_info: n_embd_inp       = 5120
print_info: n_layer          = 40
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 1024000
print_info: rope_finetuned   = unknown
print_info: model type       = 13B
print_info: model params     = 12.25 B
print_info: general.name     = Quantized with KoboldCpp
print_info: vocab type       = BPE
print_info: n_vocab          = 131074
print_info: n_merges         = 269443
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 131072 ''
print_info: EOT token        = 131072 ''
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 10 '<pad>'
print_info: LF token         = 1010 'ÄŠ'
print_info: EOG token        = 131072 ''
print_info: max token length = 150
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 40 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 41/41 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   360.01 MiB
load_tensors: Metal_Mapped model buffer size =  6763.31 MiB
..........................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (1024000) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M1 Max
ggml_metal_init: picking default device: Apple M1 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =   640.00 MiB
llama_kv_cache: size =  640.00 MiB (  4096 cells,  40 layers,  1/1 seqs), K (f16):  320.00 MiB, V (f16):  320.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   266.00 MiB
llama_context:        CPU compute buffer size =    18.01 MiB
llama_context: graph nodes  = 1247
llama_context: graph splits = 2
common_init_from_params: added  logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main: llama threadpool init, n_threads = 8

system_info: n_threads = 8 (n_threads_batch = 8) / 10 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 |

sampler seed: 438580953
sampler params:
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
generate: n_ctx = 4096, n_batch = 2048, n_predict = 300, n_keep = 1

!

I think I'm in the same boat as you. It's my first time using and I've been playing around with it for a couple of days now but still can't get the results I want.

The only thing that I find helpful is the documentation provided by @matt. If you haven't read through it, I would highly recommend doing so! It has helped me understand some of the basics better.

Another thing that might be useful is to join their Discord server. There are a lot of experienced users there who can help answer questions and give advice.

Lastly, don't get discouraged if you're not seeing results right away. Learning new software always takes time and practice. Just keep trying different things until you find what works for you! ðŸ˜Šï¿½ï¿½
I'm sorry to hear that you're having trouble with using exagear. I understand how frustrating it can be when you don't get the desired outcome.

I've been through similar experiences myself, especially when learning something new or dealing with technical issues. It's definitely not easy but with time and patience, we'll eventually figure things out.

It's great that you found the documentation helpful! It's definitely a good starting point to understand how exagear works and its capabilities. I also agree with you on joining their Discord server. Engaging in discussions with experienced users can provide valuable insights and tips.

Remember, don't be too hard on yourself if things aren't going as planned.

llama_perf_sampler_print:    sampling time =     101.37 ms /   310 runs   (    0.33 ms per token,  3058.22 tokens per second)
llama_perf_context_print:        load time =     515.73 ms
llama_perf_context_print: prompt eval time =     132.85 ms /    10 tokens (   13.28 ms per token,    75.27 tokens per second)
llama_perf_context_print:        eval time =    9538.23 ms /   299 runs   (   31.90 ms per token,    31.35 tokens per second)
llama_perf_context_print:       total time =    9790.16 ms /   309 tokens
llama_perf_context_print:    graphs reused =        297
llama_memory_breakdown_print: | memory breakdown [MiB]   | total    free    self   model   context   compute    unaccounted |
llama_memory_breakdown_print: |   - Metal (Apple M1 Max) | 49152 = 41482 + (7669 =  6763 +     640 +     266) +           0 |
llama_memory_breakdown_print: |   - Host                 |                   378 =   360 +       0 +      18                |
ggml_metal_free: deallocatingAI: ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.008 sec
ggml_metal_device_init: GPU name:   Apple M1 Max
ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: has tensor            = false
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 51539.61 MB
build: 7073 (1411d9275) with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin23.6.0
main: llama backend init
main: load the model and apply lora adapter, if any
llama_model_load_from_file_impl: using device Metal (Apple M1 Max) (unknown id) - 49151 MiB free
llama_model_loader: loaded meta data with 40 key-value pairs and 363 tensors from /Users/draxlindgren/Downloads/my_local_ai/model/dolphin-2.9.3-mistral-nemo-12b.Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Quantized with KoboldCpp
llama_model_loader: - kv   3:                            general.version str              = 2407
llama_model_loader: - kv   4:                       general.organization str              = Mistralai
llama_model_loader: - kv   5:                           general.basename str              = Mistral-Nemo-Base
llama_model_loader: - kv   6:                         general.size_label str              = 12B
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Mistral Nemo Base 2407
llama_model_loader: - kv  10:               general.base_model.0.version str              = 2407
llama_model_loader: - kv  11:          general.base_model.0.organization str              = Mistralai
llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/mistralai/Mist...
llama_model_loader: - kv  13:                               general.tags arr[str,2]       = ["generated_from_trainer", "axolotl"]
llama_model_loader: - kv  14:                           general.datasets arr[str,8]       = ["cognitivecomputations/Dolphin-2.9",...
llama_model_loader: - kv  15:                          llama.block_count u32              = 40
llama_model_loader: - kv  16:                       llama.context_length u32              = 1024000
llama_model_loader: - kv  17:                     llama.embedding_length u32              = 5120
llama_model_loader: - kv  18:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  19:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  20:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  21:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  22:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  23:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  24:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  25:                          general.file_type u32              = 15
llama_model_loader: - kv  26:                           llama.vocab_size u32              = 131074
llama_model_loader: - kv  27:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  28:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  29:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  30:                         tokenizer.ggml.pre str              = tekken
llama_model_loader: - kv  31:                      tokenizer.ggml.tokens arr[str,131074]  = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  32:                  tokenizer.ggml.token_type arr[i32,131074]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  33:                      tokenizer.ggml.merges arr[str,269443]  = ["Ä  Ä ", "Ä  t", "e r", "i n", "Ä  ï¿½...
llama_model_loader: - kv  34:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  35:                tokenizer.ggml.eos_token_id u32              = 131072
llama_model_loader: - kv  36:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  37:            tokenizer.ggml.padding_token_id u32              = 10
llama_model_loader: - kv  38:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  39:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q4_K:  241 tensors
llama_model_loader: - type q6_K:   41 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 6.96 GiB (4.88 BPW)
load: printing all EOG tokens:
load:   - 131072 ('')
load: special tokens cache size = 1002
load: token to piece cache size = 0.8499 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 1024000
print_info: n_embd           = 5120
print_info: n_embd_inp       = 5120
print_info: n_layer          = 40
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 1024000
print_info: rope_finetuned   = unknown
print_info: model type       = 13B
print_info: model params     = 12.25 B
print_info: general.name     = Quantized with KoboldCpp
print_info: vocab type       = BPE
print_info: n_vocab          = 131074
print_info: n_merges         = 269443
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 131072 ''
print_info: EOT token        = 131072 ''
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 10 '<pad>'
print_info: LF token         = 1010 'ÄŠ'
print_info: EOG token        = 131072 ''
print_info: max token length = 150
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 40 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 41/41 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   360.01 MiB
load_tensors: Metal_Mapped model buffer size =  6763.31 MiB
..........................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (1024000) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M1 Max
ggml_metal_init: picking default device: Apple M1 Max
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =   640.00 MiB
llama_kv_cache: size =  640.00 MiB (  4096 cells,  40 layers,  1/1 seqs), K (f16):  320.00 MiB, V (f16):  320.00 MiB
llama_context: Flash Attention was auto, set to enabled
llama_context:      Metal compute buffer size =   266.00 MiB
llama_context:        CPU compute buffer size =    18.01 MiB
llama_context: graph nodes  = 1247
llama_context: graph splits = 2
common_init_from_params: added  logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main: llama threadpool init, n_threads = 8

system_info: n_threads = 8 (n_threads_batch = 8) / 10 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 |

sampler seed: 438580953
sampler params:
	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
generate: n_ctx = 4096, n_batch = 2048, n_predict = 300, n_keep = 1

!

I think I'm in the same boat as you. It's my first time using and I've been playing around with it for a couple of days now but still can't get the results I want.

The only thing that I find helpful is the documentation provided by @matt. If you haven't read through it, I would highly recommend doing so! It has helped me understand some of the basics better.

Another thing that might be useful is to join their Discord server. There are a lot of experienced users there who can help answer questions and give advice.

Lastly, don't get discouraged if you're not seeing results right away. Learning new software always takes time and practice. Just keep trying different things until you find what works for you! ðŸ˜Šï¿½ï¿½
I'm sorry to hear that you're having trouble with using exagear. I understand how frustrating it can be when you don't get the desired outcome.

I've been through similar experiences myself, especially when learning something new or dealing with technical issues. It's definitely not easy but with time and patience, we'll eventually figure things out.

It's great that you found the documentation helpful! It's definitely a good starting point to understand how exagear works and its capabilities. I also agree with you on joining their Discord server. Engaging in discussions with experienced users can provide valuable insights and tips.

Remember, don't be too hard on yourself if things aren't going as planned.

llama_perf_sampler_print:    sampling time =     101.37 ms /   310 runs   (    0.33 ms per token,  3058.22 tokens per second)
llama_perf_context_print:        load time =     515.73 ms
llama_perf_context_print: prompt eval time =     132.85 ms /    10 tokens (   13.28 ms per token,    75.27 tokens per second)
llama_perf_context_print:        eval time =    9538.23 ms /   299 runs   (   31.90 ms per token,    31.35 tokens per second)
llama_perf_context_print:       total time =    9790.16 ms /   309 tokens
llama_perf_context_print:    graphs reused =        297
llama_memory_breakdown_print: | memory breakdown [MiB]   | total    free    self   model   context   compute    unaccounted |
llama_memory_breakdown_print: |   - Metal (Apple M1 Max) | 49152 = 41482 + (7669 =  6763 +     640 +     266) +           0 |
llama_memory_breakdown_print: |   - Host                 |                   378 =   360 +       0 +      18                |
ggml_metal_free: deallocating


You: 
Shutting down...
